{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oyd_gfPfLU9",
        "outputId": "584b946c-a3ab-4fca-feae-b62b7822a9e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: what is photosynthesis\n",
            "Teacher: I'm excited to explore the topic of photosynthesis with you. Photosynthesis is a vital process that occurs in plants, algae, and some bacteria. It's the way they produce their own food using energy from the sun.\n",
            "\n",
            "To start, can you tell me what you already know about photosynthesis? What comes to mind when you hear the word \"photosynthesis\"? \n",
            "\n",
            "(Also, don't worry if you're not sure or don't know much about it yet. We'll learn and discover together.)\n",
            "Student: That sounds fascinating!  When I hear \"photosynthesis,\" I think of plants using sunlight to grow.  I'm curious,  **what are the main ingredients that plants need besides sunlight to make their food?** \n",
            "\n",
            "\n",
            "Teacher: That's a great start. You're on the right track by thinking of plants using sunlight to grow. Now, let's dive deeper into the process of photosynthesis. \n",
            "\n",
            "You asked about the main ingredients that plants need besides sunlight to make their food. Can you take a guess? What do you think plants might need to undergo photosynthesis, besides sunlight?\n",
            "\n",
            "Also, have you ever noticed that plants typically grow near water sources or in soil with good drainage? Do you think water might play a role in the process of photosynthesis?\n",
            "Student: That's really interesting! I was thinking plants need water and maybe air too.  Do you think the air is important for photosynthesis, and if so, what part of the air do you think plants use? \n",
            "\n",
            "Teacher: You're on the right track. Plants do need water and air to survive. Now, let's dive deeper into the role of air in plant growth. \n",
            "\n",
            "You mentioned that air might be important for photosynthesis. That's a great connection to make. Photosynthesis is the process by which plants make their own food from sunlight, water, and... something in the air. \n",
            "\n",
            "Can you think of what specific component in the air might be crucial for photosynthesis? Is it the oxygen, nitrogen, or maybe something else? What do you think plants use from the air to help them make food?\n",
            "Continue conversation? (or type a new question): quit\n",
            "Good Bye!\n",
            "User: quit\n",
            "Good Bye!\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Define two LLMs\n",
        "teacher_llm = ChatGroq(groq_api_key=\"\", model_name=\"llama-3.3-70b-versatile\")\n",
        "student_llm = ChatGroq(groq_api_key=\"\", model_name=\"Gemma2-9b-It\")\n",
        "\n",
        "# Define state\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Teacher function\n",
        "def teacher(state: State):\n",
        "    teacher_prompt = f\"You are a teacher. Guide the student and ask questions based on: {state['messages'][-1].content}\"\n",
        "    teacher_response = teacher_llm.invoke([HumanMessage(content=teacher_prompt)])\n",
        "    return {\"messages\": [AIMessage(content=teacher_response.content)]}  # Use AIMessage for teacher\n",
        "\n",
        "# Student function\n",
        "def student(state: State):\n",
        "    student_prompt = f\"You are a student. Respond to the teacher and ask a follow-up question based on: {state['messages'][-1].content}\"\n",
        "    student_response = student_llm.invoke([HumanMessage(content=student_prompt)])\n",
        "    return {\"messages\": [HumanMessage(content=student_response.content)]}  # Use HumanMessage for student\n",
        "\n",
        "# Add nodes\n",
        "graph_builder.add_node(\"teacher\", teacher)\n",
        "graph_builder.add_node(\"student\", student)\n",
        "\n",
        "# Define flow\n",
        "graph_builder.add_edge(START, \"teacher\")\n",
        "graph_builder.add_edge(\"teacher\", \"student\")\n",
        "graph_builder.add_edge(\"student\", \"teacher\")  # Loop continues\n",
        "\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "# Conversation loop with a recursion limit\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"q\"]:\n",
        "        print(\"Good Bye!\")\n",
        "        break  # Exits the while loop correctly\n",
        "\n",
        "    state = {\"messages\": [HumanMessage(content=user_input)]}\n",
        "    turn_count = 0\n",
        "\n",
        "    for event in graph.stream(state):\n",
        "        for value in event.values():\n",
        "            message = value[\"messages\"][0]\n",
        "            if isinstance(message, AIMessage):\n",
        "                print(f\"Teacher: {message.content}\")  # Print as Teacher\n",
        "            else:\n",
        "                print(f\"Student: {message.content}\")  # Print as Student\n",
        "\n",
        "            turn_count += 1\n",
        "            if turn_count >= 5:\n",
        "                user_input = input(\"Continue conversation? (or type a new question): \")\n",
        "                if user_input.lower() in [\"quit\", \"q\"]:\n",
        "                    print(\"Good Bye!\")\n",
        "                    break  # Exits the inner loop\n",
        "\n",
        "        if user_input.lower() in [\"quit\", \"q\"]:\n",
        "            break  # Ensures we also exit the outer while loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "import time\n",
        "\n",
        "# Define your Groq API key\n",
        "GROQ_API_KEY = \"\"\n",
        "\n",
        "# Define two LLMs\n",
        "teacher_llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=\"llama-3.3-70b-versatile\")\n",
        "student_llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=\"Gemma2-9b-It\")\n",
        "\n",
        "# Token limits based on Groq's rate limits\n",
        "TOKEN_LIMITS = {\n",
        "    \"llama-3.3-70b-versatile\": {\"TPM\": 6000, \"TPD\": 100000},\n",
        "    \"Gemma2-9b-It\": {\"TPM\": 15000, \"TPD\": 500000}\n",
        "}\n",
        "\n",
        "# Token tracking variables\n",
        "total_tokens_used = 0\n",
        "tokens_used_last_minute = 0\n",
        "start_time = time.time()\n",
        "\n",
        "# Function to estimate token count\n",
        "def count_tokens(text):\n",
        "    return len(text.split())  # Approximate token count using word count\n",
        "\n",
        "# Function to check token limits and warn\n",
        "def check_token_limits(tokens_used, model_name):\n",
        "    global total_tokens_used, tokens_used_last_minute\n",
        "    total_tokens_used += tokens_used\n",
        "    tokens_used_last_minute += tokens_used\n",
        "\n",
        "    tpm_limit = TOKEN_LIMITS[model_name][\"TPM\"]\n",
        "    tpd_limit = TOKEN_LIMITS[model_name][\"TPD\"]\n",
        "\n",
        "    tokens_available_tpm = tpm_limit - tokens_used_last_minute\n",
        "    tokens_available_tpd = tpd_limit - total_tokens_used\n",
        "\n",
        "    # Reset per-minute counter every 60 seconds\n",
        "    if time.time() - start_time >= 60:\n",
        "        tokens_used_last_minute = 0\n",
        "\n",
        "    # Display available tokens\n",
        "    print(f\"\\n(Tokens Used: {tokens_used}, Total: {total_tokens_used}, Available (TPM: {tokens_available_tpm}, TPD: {tokens_available_tpd}))\")\n",
        "\n",
        "    # Warnings\n",
        "    if tokens_used_last_minute >= 0.8 * tpm_limit:\n",
        "        print(\"WARNING: You are close to your Tokens Per Minute (TPM) limit.\")\n",
        "\n",
        "    if total_tokens_used >= 0.8 * tpd_limit:\n",
        "        print(\"WARNING: You are close to your Tokens Per Day (TPD) limit.\")\n",
        "\n",
        "# Teacher function\n",
        "def teacher(state: State):\n",
        "    teacher_prompt = f\"You are a teacher. Guide the student and ask questions based on: {state['messages'][-1].content}\"\n",
        "    teacher_response = teacher_llm.invoke([HumanMessage(content=teacher_prompt)])\n",
        "\n",
        "    tokens_used = count_tokens(teacher_response.content)\n",
        "    check_token_limits(tokens_used, \"llama-3.3-70b-versatile\")\n",
        "\n",
        "    return {\"messages\": [AIMessage(content=teacher_response.content)]}  # Use AIMessage for teacher\n",
        "\n",
        "# Student function\n",
        "def student(state: State):\n",
        "    student_prompt = f\"You are a student. Respond to the teacher and ask a follow-up question based on: {state['messages'][-1].content}\"\n",
        "    student_response = student_llm.invoke([HumanMessage(content=student_prompt)])\n",
        "\n",
        "    tokens_used = count_tokens(student_response.content)\n",
        "    check_token_limits(tokens_used, \"Gemma2-9b-It\")\n",
        "\n",
        "    return {\"messages\": [HumanMessage(content=student_response.content)]}  # Use HumanMessage for student\n",
        "\n",
        "# Define state\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Add nodes\n",
        "graph_builder.add_node(\"teacher\", teacher)\n",
        "graph_builder.add_node(\"student\", student)\n",
        "\n",
        "# Define flow\n",
        "graph_builder.add_edge(START, \"teacher\")\n",
        "graph_builder.add_edge(\"teacher\", \"student\")\n",
        "graph_builder.add_edge(\"student\", \"teacher\")  # Loop continues\n",
        "\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "# Conversation loop with a recursion limit\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"q\"]:\n",
        "        print(\"\\nGoodbye!\")\n",
        "        break  # Exits the while loop correctly\n",
        "\n",
        "    state = {\"messages\": [HumanMessage(content=user_input)]}\n",
        "    turn_count = 0\n",
        "\n",
        "    for event in graph.stream(state):\n",
        "        for value in event.values():\n",
        "            message = value[\"messages\"][0]\n",
        "            if isinstance(message, AIMessage):\n",
        "                print(f\"\\nTeacher: {message.content}\")  # Print as Teacher\n",
        "            else:\n",
        "                print(f\"\\nStudent: {message.content}\")  # Print as Student\n",
        "\n",
        "            turn_count += 1\n",
        "            if turn_count >= 5:\n",
        "                user_input = input(\"Continue conversation? (or type a new question): \")\n",
        "                if user_input.lower() in [\"quit\", \"q\"]:\n",
        "                    print(\"\\nGoodbye!\")\n",
        "                    break  # Exits the inner loop\n",
        "\n",
        "        if user_input.lower() in [\"quit\", \"q\"]:\n",
        "            break  # Ensures we also exit the outer while loop\n",
        "\n",
        "# Display final token stats after quitting\n",
        "tokens_available_tpd = TOKEN_LIMITS[\"llama-3.3-70b-versatile\"][\"TPD\"] - total_tokens_used\n",
        "print(\"\\nFinal Token Usage:\")\n",
        "print(f\"   Total Tokens Used: {total_tokens_used}\")\n",
        "print(f\"   Tokens Available (TPD): {tokens_available_tpd} (out of {TOKEN_LIMITS['llama-3.3-70b-versatile']['TPD']})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OHKQaB6fmVv",
        "outputId": "0363768d-b661-492a-d6b3-113789989517"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: what is photosynthesis\n",
            "\n",
            "(Tokens Used: 46, Total: 46, Available (TPM: 5954, TPD: 99954))\n",
            "\n",
            "Teacher: I'd love to explore the concept of photosynthesis with you. To start, can you tell me what you've heard about photosynthesis before? Maybe you've learned about it in school or heard about it from someone else. What comes to mind when you hear the word \"photosynthesis\"?\n",
            "\n",
            "(Tokens Used: 48, Total: 94, Available (TPM: 14906, TPD: 499906))\n",
            "\n",
            "Student: That sounds great! When I hear the word \"photosynthesis,\" I think about plants using sunlight to make their own food.  I remember learning about chlorophyll and how it absorbs light energy, but I'm curious, what exactly is the food that plants make, and how do they use it? \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(Tokens Used: 127, Total: 221, Available (TPM: 5779, TPD: 99779))\n",
            "\n",
            "Teacher: You're on the right track by thinking about plants using sunlight to make their own food through photosynthesis. That's a great foundation to build on.\n",
            "\n",
            "To dive deeper, let's explore what you mentioned about chlorophyll absorbing light energy. Can you tell me more about what you understand about chlorophyll and its role in photosynthesis? What do you think happens to the light energy once it's absorbed by chlorophyll?\n",
            "\n",
            "Also, you asked a great question about the food that plants make. Do you have any ideas about what that food might be, or how plants might use it to grow and develop? \n",
            "\n",
            "Let's break it down step by step. What do you think is the main product of photosynthesis, and how do plants utilize it to sustain themselves?\n",
            "\n",
            "(Tokens Used: 77, Total: 298, Available (TPM: 14702, TPD: 499702))\n",
            "\n",
            "Student: That's really helpful, thanks!  \n",
            "\n",
            "I understand that chlorophyll is what gives plants their green color, and it helps them absorb sunlight.  But what I'm not sure about is *how* it actually uses that light energy. Does it change the light into something else?\n",
            "\n",
            "And about the food plants make, I think it might be some kind of sugar, since sugar gives us energy. Do you think plants use that sugar to build their leaves, stems, and roots? \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(Tokens Used: 167, Total: 465, Available (TPM: 5535, TPD: 99535))\n",
            "\n",
            "Teacher: You're on the right track.  Chlorophyll does help plants absorb sunlight, and that energy is used to power a process called photosynthesis. \n",
            "\n",
            "To clarify, when you say \"change the light into something else,\" are you thinking that the light energy is being converted into a different form of energy? If so, you're correct. The light energy is used to convert carbon dioxide and water into glucose (a type of sugar) and oxygen.\n",
            "\n",
            "Regarding the food plants make, you're correct again that it's a type of sugar, specifically glucose. Plants use this glucose as energy and building blocks to construct their tissues, such as leaves, stems, and roots. But how do you think the plants use the glucose to build these tissues? Is it similar to how our bodies use energy from the food we eat? \n",
            "\n",
            "Also, what do you think happens to the oxygen that's produced during photosynthesis? Is it just released into the air, or does it have any other role in the plant's life cycle?\n",
            "Continue conversation? (or type a new question): quit\n",
            "\n",
            "Goodbye!\n",
            "User: quit\n",
            "\n",
            "Goodbye!\n",
            "\n",
            "Final Token Usage:\n",
            "   Total Tokens Used: 465\n",
            "   Tokens Available (TPD): 99535 (out of 100000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Define your Groq API key\n",
        "GROQ_API_KEY = \"\"\n",
        "\n",
        "# Define two LLMs\n",
        "teacher_llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=\"llama-3.3-70b-versatile\")\n",
        "student_llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=\"Gemma2-9b-It\")\n",
        "\n",
        "# Token limits based on Groq's rate limits\n",
        "TOKEN_LIMITS = {\n",
        "    \"llama-3.3-70b-versatile\": {\"TPM\": 6000, \"TPD\": 100000},\n",
        "    \"Gemma2-9b-It\": {\"TPM\": 15000, \"TPD\": 500000}\n",
        "}\n",
        "\n",
        "# Token tracking variables\n",
        "total_tokens_used = 0\n",
        "\n",
        "# Function to estimate token count\n",
        "def count_tokens(text):\n",
        "    return len(text.split())  # Approximate token count using word count\n",
        "\n",
        "# Function to check token limits\n",
        "def check_token_limits(tokens_used, model_name):\n",
        "    global total_tokens_used\n",
        "    total_tokens_used += tokens_used\n",
        "    tokens_available_tpd = TOKEN_LIMITS[model_name][\"TPD\"] - total_tokens_used\n",
        "    return tokens_available_tpd\n",
        "\n",
        "# Teacher function\n",
        "def teacher(state):\n",
        "    last_message = state[\"messages\"][-1].content\n",
        "\n",
        "    # Check if the user wants to exit\n",
        "    if \"goodbye\" in last_message.lower():\n",
        "        return {\"messages\": [AIMessage(content=\"Goodbye! Have a great day.\")], \"end\": True}\n",
        "\n",
        "    teacher_prompt = f\"You are a teacher. Guide the student and ask questions based on: {last_message}\"\n",
        "    teacher_response = teacher_llm.invoke([HumanMessage(content=teacher_prompt)])\n",
        "\n",
        "    tokens_used = count_tokens(teacher_response.content)\n",
        "    check_token_limits(tokens_used, \"llama-3.3-70b-versatile\")\n",
        "\n",
        "    return {\"messages\": [AIMessage(content=teacher_response.content)]}\n",
        "\n",
        "# Student function\n",
        "def student(state):\n",
        "    last_message = state[\"messages\"][-1].content\n",
        "\n",
        "    # Check if the user wants to exit\n",
        "    if \"goodbye\" in last_message.lower():\n",
        "        return {\"messages\": [HumanMessage(content=\"Goodbye! Thank you!\")], \"end\": True}\n",
        "\n",
        "    student_prompt = f\"You are a student. Respond to the teacher and ask a follow-up question based on: {last_message}\"\n",
        "    student_response = student_llm.invoke([HumanMessage(content=student_prompt)])\n",
        "\n",
        "    tokens_used = count_tokens(student_response.content)\n",
        "    check_token_limits(tokens_used, \"Gemma2-9b-It\")\n",
        "\n",
        "    return {\"messages\": [HumanMessage(content=student_response.content)]}\n",
        "\n",
        "# Define state\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# Build the graph\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_node(\"teacher\", teacher)\n",
        "graph_builder.add_node(\"student\", student)\n",
        "\n",
        "graph_builder.add_edge(START, \"teacher\")\n",
        "graph_builder.add_edge(\"teacher\", \"student\")\n",
        "graph_builder.add_edge(\"student\", \"teacher\")  # Loop continues\n",
        "\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "# Chat Interface Function\n",
        "def chat_interface(user_input):\n",
        "    messages = []\n",
        "    state = {\"messages\": [HumanMessage(content=user_input)]}\n",
        "    turn_count = 0\n",
        "\n",
        "    for event in graph.stream(state, config={\"recursion_limit\": 10}):  # Set recursion limit\n",
        "        for value in event.values():\n",
        "            message = value[\"messages\"][0]\n",
        "            role = \"teacher\" if isinstance(message, AIMessage) else \"student\"\n",
        "\n",
        "            # Append message with the correct alignment\n",
        "            messages.append((role, message.content))\n",
        "\n",
        "            turn_count += 1\n",
        "            if turn_count >= 5 or value.get(\"end\"):  # Stop condition\n",
        "                return messages\n",
        "\n",
        "    return messages\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"Teacher-Student Chatbot using langgraph\")\n",
        "    chatbot = gr.Chatbot(label=\"Teacher-Student Conversation\", bubble_full_width=False)  # Make sure bubbles are not full width\n",
        "    textbox = gr.Textbox(placeholder=\"Type your message...\")\n",
        "    submit = gr.Button(\"Send\")\n",
        "\n",
        "    def update_chat(user_input, chat_history):\n",
        "        chat_responses = chat_interface(user_input)\n",
        "\n",
        "        for role, message in chat_responses:\n",
        "            if role == \"teacher\":\n",
        "                chat_history.append((None, message))  # Teacher messages on the right\n",
        "            else:\n",
        "                chat_history.append((message, None))  # Student messages on the left\n",
        "\n",
        "        return chat_history, \"\"\n",
        "\n",
        "    submit.click(update_chat, [textbox, chatbot], [chatbot, textbox])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "PbFLBpGvfsRh",
        "outputId": "32a3a437-3a2a-445c-dac8-8febc8567ca4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-c3d165689c17>:106: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Teacher-Student Conversation\", bubble_full_width=False)  # Make sure bubbles are not full width\n",
            "<ipython-input-8-c3d165689c17>:106: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(label=\"Teacher-Student Conversation\", bubble_full_width=False)  # Make sure bubbles are not full width\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://91f905766a2f9df270.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://91f905766a2f9df270.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NBpzBT51gInK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}